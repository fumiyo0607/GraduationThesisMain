\chapter{準備}
本章では，まず一般的な自動分類問題について説明した後，本研究で扱う半教師あり学習について述べる．さらに本研究で用いる二値判別器を説明し，二値分類問題を多値判別問題に拡張する手法であるECOC法について述べる．

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{自動分類問題}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\label{sec:zirei}
自動分類問題\cite{bis1},\;\cite{bis2}とは，カテゴリ情報があらかじめ与えられた学習データから分類規則を学習し，カテゴリが未知の入力データのカテゴリを推定する問題である \cite{Nishida08}．
ここで，${\bm x} \in \mathbb{R}^{d}$を$d$次元特徴ベクトル，$\mathcal C = \{c_1,...,c_k,...,c_K\}$をカテゴリラベルの集合，$y\in \mathcal C$とすると，$N$個の学習データ集合$\mathcal D = \{{\bm x}_{n},y_{n}\}^{N}_{n=1}$をもとに${\bm x}$のカテゴリラベルを$K$個のカテゴリ$\mathcal C$の中から1つに決定する問題である．
すなわち，これを達成するための関数$f:{\bm x}→y$を学習するための問題であるといえる．また，以降で扱う二値分類問題では，$K = 2$でありカテゴリ集合$y$は$\{+1,-1\}$とする．

データの分類は一般的に以下の処理を行うことにより実現される．

(1)特徴抽出

(2)分類規則の学習

(3)データの分類
% 分類問題文献
%\bibitem{Nishida08}
%K. Nishida, \lq \lq Learning and detecting concept drift,'' \textit{researchgate.net}, 2008

%そして，「特徴の抽出」と「分類器の学習」，「分類器による予測」の3段階に分かれる．
%

特徴抽出とは，分類に用いる各データの特徴量を構成するための処理である．すなわち，機械による自動処理が可能になるようにデータの特徴量を数値化しベクトルに変換することで，特徴ベクトルとして表現することである．

分類規則の学習とは，入力データの特徴ベクトルとそのカテゴリをセットにした学習データを用いて，新規入力データがどのカテゴリに所属するかを決定するための規則を学習することである．分類規則は，同一カテゴリに属するデータの特徴を捉え学習される．

データの分類とは，カテゴリが未知の新規入力データを，学習をして得た分類規則に従い一つのカテゴリに分類することである．
%カテゴリが未知の新規入力データ${\bm x}_{new}$の所属カテゴリを推定する．
\par


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{半教師あり学習}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\label{sec:taisyoudata}
一般的に分類問題を扱う際に行われる学習は，教師あり学習と呼ばれるものである．教師あり学習では全ての学習データにおいて，特徴ベクトルとそれに対応するカテゴリラベルが与えられている．例えば，動物の写った画像データが与えられたとき，全ての各画像に対して，｛イヌ，ネコ，ゾウ，．．．｝といったカテゴリの内のいずれかの適切なカテゴリが対応した状態である．\par
教師あり学習は分類問題を扱う際，学習データが多いとき良い分類境界の学習を行うことができる．しかしながら実世界に存在する多くのデータにはカテゴリが付与されていない．大量のデータにラベルを付与する作業は，時間，費用の二つの面において大きな問題点となっている．一方，少数のラベルありデータのみを学習した場合，データ数が少ないためそのデータに依存しやすく過学習してしまう．\par
そこでこれらの問題点を解消する手法として半教師あり学習が近年注目されている．半教師あり学習では，少数のラベルありデータと多数のラベルなしデータを用いることで分類規則を構築する手法である．この手法は，多数のラベルなしデータを用いることで本来望まれる分類境界に近づけることを目的とした手法である．


\vspace{2.0mm}
\begin{figurehere}
\begin{center}
  \includegraphics[width=10.0cm]{hankyoshi.png}\par
\caption{機械学習の種類\cite{semi}}
\label{機械学習の種類}
\end{center}\par
\end{figurehere}
%\vspace{1.0mm}
\newpage
%本論文では，特徴ベクトル$\bm{x}\in\mathbb{R}^d$のカテゴリラベルを$K$個のカテゴリ$\mathcal C =\{c_1,...,c_k,...,c_K\}$の中から1つに決定する多値分類問題を扱う．
半教師あり学習では，ラベルありデータとラベルなしデータを扱うためそれぞれ別のデータ集合として定義する．ラベルありデータ集合は$\mathcal D_L = \{\bm{x}_i^L,y_i^L\}_{i=1}^{N_L}$と定義し，特徴ベクトル$\bm{x}_i^L$とそれに対応したカテゴリラベル$y_i^L$が与えられている．ラベルなしデータ集合を$\mathcal D_U = \{\bm{x}_i^U,*\}_{i=1}^{N_U}$と記述し，$*$は未観測であることを表す．すなわち，特徴ベクトル$\bm{x}_i^U$に対するカテゴリラベルが与えられていない．ここで，$\bm{x}_i^L\in\mathbb{R}^d$，$\bm{x}_i^U\in\mathbb{R}^d$，ラベルありデータ数を$N_L$，ラベルなしデータ数を$N_U$とする．また，半教師あり学習は少数のラベルありデータと多数のラベルなしデータを用いるため，一般に$N_L\ll N_U$を前提とする．



%少数のラベルありデータ集合$\mathcal D_L = \{\bm{x}_i^L,y_i^L\}_{i=1}^{N_L}$と多数のラベルなしデータ集合$\mathcal D_U = \{\bm{x}_i^U,*\}_{i=1}^{N_U}$を用いて学習を行う．ここで，$\bm{x}_i^L\in\mathbb{R}^d$，$\bm{x}_i^U\in\mathbb{R}^d$とする．ラベルありデータ数を$N_L$，ラベルなしデータ数を$N_U$とし，一般に$N_L\ll N_U$を前提とする．

ラベルありデータ集合は$\mathcal D_L$を用いてラベルなしデータ$\bm{x}_i^U$のラベル$y_i^U$を推定し，仮のラベル(以下，仮ラベル)を付与することで，これら全てのデータを用いて分類器の学習を行う．
%カテゴリが未知の入力データに対し，所属カテゴリを推定する．
%$\tilde{\vector{x}}$$\tilde{y}\in \mathcal C$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{二値判別器}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
本節では，本研究において用いる学習器について述べる．
\subsection{Support Vector Machine}
Support Vector Machine(以下，SVM） \cite{VV}は1995年にV.Vapnikにより考案された二値分類手法で，強力な汎化能力を持った手法として現在でも広く応用されている．SVMは，クラス間マージンの最大化を行うことで分離超平面を得る識別関数の学習法である \cite{Bryant99}．マージンの最大化は，学習データにより与えられた不等式制約条件下で最適化問題を解くことにより得られる．

学習データ${\bm x}_{n}\in \mathbb{R}^{d}$の所属するカテゴリが正例ならば$y_{n}=+1$，負例ならば$y_{n}=-1$であるとき，各カテゴリの学習データが線形分離可能であると仮定すると，各データ${\bm x}_{n}$に対して，次の式が成り立つ．

\begin{eqnarray}
f({\bm x}_{n})={\bm w}^{\mathrm{T}}{\bm x}_{n}+b
\end{eqnarray}
\begin{eqnarray}
y_{n}=
\begin{cases}
+1 &f({\bm x}_{n})\ge \kappa \\
-1 & f({\bm x}_{n})< \kappa 
\end{cases}
\end{eqnarray}
%y_{n}=
ここで，$f(\cdot)$は分類器，${\bm w}=(w_{1},w_{2},...,w_{d})^{\mathrm{T}}$は係数ベクトル，$b$はバイアス項，$\kappa$はマージンを表す．ここでマージンとは，分類器から得られる識別超平面とその超平面に対して最近傍の学習データ(サポートベクトル)との距離のことを示す．
%，${\bm w}^{T}{\bm x}_{n}$は係数ベクトルと各学習データの内積
式(2.2)をマージン$\kappa$で正規化し，正規化した係数ベクトルとバイアス項をそれぞれ${\bm w}$と$b$で再定義すると$f({\bm x}_{n})$は式(2.3)と表すことができる．

\begin{eqnarray}
f({\bm x}_{n})={\bm w}^{\mathrm{T}}{\bm x}_{n}+b
\end{eqnarray}
\begin{eqnarray}
y_{n}=
\begin{cases}
+1 &f({\bm x}_{n})\ge \kappa \\
-1 & f({\bm x}_{n})< \kappa 
\end{cases}
\end{eqnarray}

この式は，
\begin{eqnarray}
y_{n}({\bm w}^{\mathrm{T}}{\bm x}_{n}+b)-1\ge0 ,\;\;n=1,...,N
\end{eqnarray}
と表すことができる．

式(2.3)を満たす超平面は無数に存在する．このとき，最大マージンを持つ分類器から構築される超平面を最適分離超平面と呼び，以下に示す最適化問題を解くことにより，最適分離超平面を構築する分類器を求める．\par
%\newpage
{\bf 最適化問題}
\begin{eqnarray}
\text{minimize} & \frac{1}{2} {\bm w}^{\mathrm{T}}{\bm w} \nonumber \\
\text{subject\;\;to} & y_{n}({\bm w}^{\mathrm{T}}{\bm x}_{n}+b)-1\le0
\end{eqnarray}

式(2.4)は制約付き最適化問題であるので，ラグランジュ未定乗数法 \cite{Platt98}を用いて解くことができる．各不等式の制約に対応する未定乗数${\bm a}=(a_{1},a_{2},...,a_{N})^{\mathrm{T}}\;(a_{n}\le 1)$を用いてラグランジュ関数を
\begin{eqnarray}
L({\bm w},{\bm a})=\frac{1}{2}||{\bm w}||^{2}+\sum^{N}_{n=1}a_{n}(1-y_{n}(w_{0}+{\bm w}^{\mathrm{T}}{\bm x}_{n}))
\end{eqnarray}
と定式化することができる．

これを用いて，以下の式（2.8）で定義される双対問題を解くことにより最適化を得る．
\begin{eqnarray}
\text{maximize} & \sum^{N}_{n=1}\alpha_n - \frac{1}{2}\sum^{N}_{n=1}\sum^{N}_{m=1}\alpha_{n}\alpha_{m}y_{n}y_{m}{\bm x}_{n}^{T}{\bm x}_{m}\nonumber \\
\text{subject\;\;to} & \sum^{N}_{n=1}\alpha_{n}t_{n}=0
\end{eqnarray}


以下にSVMの分類境界を表す例をあげる．
\begin{figure}[H]
\centering
\includegraphics[width=10.0cm]{svm.png}
\caption{SVMの分類境界}
\label{SVM}
\end{figure}

\subsection{決定木}
決定木\cite{Safavian02},-,\cite{Wilkinson92}とは，目的変数と説明変数間の関係を木構造として捉える機械学習の一手法である．決定木は根ノードから葉ノードへ向かう途中の中間ノードで簡単な分類規則を割り当て，最終的に葉ノードに割り当てられたカテゴリを分類先とする分類器であり，単純な分類規則を複数組み合わせることで複雑な分類規則を得ることができる手法である．また，この手法は中間ノードの過程を見ることで分類に有効な特徴量や値を解析する際にも有効であると知られている．

説明変数$\bm{x}=(x_{1},x_{2})^T$が与えられたもとで予測対象の目的変数$y \in \{-1,+1\}$を予測する場合$x_1$の閾値$\theta_1$と$x_2$の閾値$\theta_2$をもとに，図\ref{kyoukaitree}に示す分類境界が得られる．

\begin{figure}[H]
\centering
\includegraphics[width=5.0cm]{kyoukaiki.png}
\caption{決定木における識別境界の例}
\label{kyoukaitree}
\end{figure}
上記の例の場合，図\ref{tree}に示すような決定木として表現できる．
\begin{figure}[H]
\centering
\includegraphics[width=6.0cm]{tree.png}
\caption{決定木}
\label{tree}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{アンサンブル学習}

複数の分類器を組み合わせることにより全体としての汎化性能の向上を図る方法としてアンサンブル学習
%\footnote{アンサンブル学習は任意の分類器によるアンサンブル手法であるが，本研究では決定木を分類器として使用するものとする．} 
\cite{Liu99},\;\cite{Ueda02},\;\cite{Ueda05}が提案されており，その有効性が示されている．アンサンブル手法の中でも，Bagging \cite{Breiman96},\;\cite{RF}，Boosting \cite{Cortes94},\;\cite{Freund97},\;\cite{Hirai12}という手法が多く用いられることが多い．以下にその概要を示す．
\subsection{Bagging}
%\begin{description}
%\item[Bagging]\mbox{}\\
\;\;\;Baggingとは，$B$個の分類器$g_{1},g_{2},...,g_{B}$ごとに学習データのブートストラップサンプル$\mathcal{D}'=\{{D}_{1},{D}_{2},...,{D}_{B}\}$を作成し，それぞれを用いて分類器$g_{1},g_{2},...,g_{B}$を独立に学習する．ここで，ブーストストラップサンプルとは，データ集合$\mathcal{D}$から重複を許し，復元抽出を行うことで学習データのサンプル$({D}_{1},{D}_{2},...,{D}_{B})$を生成する手法である \cite{Miteni99}．新規入力データ$\tilde{\bm{x}}$に対しては，それぞれの分類器出力$g_{1}(\bm{x}),g_{2}(\bm{x}),...,g_{B}(\bm{x})$の式(2.9)の多数決により推定を行う．
\begin{eqnarray}
\hat{y}={\rm sign}(g_{1}(\tilde{\bm{x}})+g_{2}(\tilde{\bm{x}})+,...,+g_{B}(\tilde{\bm{x}}))
\end{eqnarray}

ここで，{\text sign}($x$)は，$x\ge0$のとき+1，$x<0$のとき$-1$をとる関数である．以下の図\ref{ensenble}にBaggingのイメージ図を示す．


\begin{figure}[H]
\centering
\includegraphics[width=9.0cm]{bag.png}
\caption{Baggingの学習イメージ}
\label{ensenble}
\end{figure}

\newpage
\subsection{Boosting}
%\item[]\mbox{}\\ 
\;\;\;Boostingとは，以前までに構築された分類器で誤分類されたデータを正しく分類することができるように新たな分類器を構築することで，全体として分類性能を向上させる手法である．新規入力データ$\tilde{\bm{x}}$に対しては，それぞれの分類器の重み付き多数決により推定を行う．



%以下に，Boostingの代表的な手法であるAdaboostの学習法を示す．


Boostingでは，$T$個の分類器$g_{1},g_{2},...,g_{T}$を用いて，1つ前の学習器の分類結果に基づき誤分類されたとされるデータに着目し，その誤りを正しく分類できるような分類器を順に作成する．

Boostingでは，$T$個の分類器$g_{1},g_{2},...,g_{T}$と重み$\alpha_{1},\alpha_{2},...,\alpha_{T}$により$\tilde{\bm{x}}$の予測カテゴリ$\hat{y}$を
\begin{equation}
\hat{y}={\rm sign}\left(\sum^{T}_{t=1}\alpha_{t}g_{t}(\tilde{\bm{x}})\right)
\end{equation}
として求める．分類器$g_{t}({\bm x})$は，$g_{t-1}({\bm x})$で誤分類したデータに重みを付与することで，誤分類されたデータに焦点を当てた学習が行われ，これを繰り返すことで予測誤差の最小化を目指す．ここで，$t$回目の試行におけるデータに対する重みは${\bm w}_{t}=(w^{t}_{1},w^{t}_{2},...,w^{t}_{N})$となる．また，分類器$g_{t}$に対する重み$\alpha_{t}$は，分類器の誤り率が低いほど大きくなるように決定される．

図\ref{ensenble}にBoostingのイメージを示す．

\begin{figure}[H]
\centering
\includegraphics[width=8.0cm]{boost.png}
\caption{Boostingの学習イメージ}
\label{ensenble}
\end{figure}


%\end{description}

%\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ECOC法\cite{ECOC}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
複数の二値判別器を組み合わせ，多値判別器を構成する手法としてECOC法(Error-Correcting Output Cording多値判別法)が提案されている．以下では本研究で用いる手法とその復号法について述べる．

\begin{description}
\item[1-versus-the rest 多値判別手法]\mbox{}\\ 
%\subsection{1-vs-the rest 多値判別手法}
\;\;\;二値判別器を複数用いて，多値判別を行う方法の代表的手法\cite{Code}として，1-versus-the rest 多値判別手法が挙げられる．\par
1-versus-the rest 多値判別手法では，全ての$i = 1,2,...,K$に対して判別対象カテゴリ$c_i$とそれ以外のカテゴリに分ける1-versus-the rest 多値判別器を作る．\par
以下で，本手法で利用する符号表について述べる．

\begin{figure}[H]
\centering
表2.1:1-versus-the rest 多値判別法の符号表例
\includegraphics[width=10.0cm]{1vs.png}
%\label{1vs}
\end{figure}\par
%\newpage
表2.1は，カテゴリ数$K = 4$の場合における1-versus-the rest 多値判別法の符号表の例であり，各二値分類器の構成を符号表と呼ばれる$\{+1,-1\}$の二値で表される数値表により表現する．いま符号表を$\bm{W}$とすると，二値分類器の個数はカテゴリ数と同じ$K$個であるため，$\bm{W}$は$K\times K$行列となる．符号表$\bm{W}$の各列ベクトルは二値分類器の構成を表現しており，要素が$+1$のカテゴリ集合と要素が$-1$のカテゴリ集合を二値分類すると解釈できる．例えば，表2.1の1番目の列の判別器では$c_1$と$c_2,c_3,c_4$のカテゴリを分けることを意味する．また，符号表$\bm{W}$の$k$行目の行ベクトルをカテゴリ$c_k$の符号語と呼び$\bm{W}_{c_k}$と表現する．
以下では，本研究で使用するハミング距離を用いた復号法を説明する．



\item[ECOC法に基づく復号法]\mbox{}\\ 
%\subsection{1-vs-the rest 多値判別手法}
\;\;\;二値判別器を複数用いて多値判別を行うが，そのときの復号方法について述べる．また，二値判別器の出力が確率値として出力される場合，分類結果のカテゴリとして表される場合があるためそれぞれについて説明する．


新規入力データ$\tilde{\bm{x}}$に対する$r (1\leq r \leq R)$番目の二値分類器の出力を$g_r(\tilde{\bm{x}})\in\{+1,-1\}$，符号語$\bm{W}_{c_k}$の$r$番目の値を$W_{c_k\cdot r}$と定義する．このとき$\bm{x}$は，符号語$\bm{W}_{c_k}$と分類器の出力$\bm{g}=(g_1(\bm{x}),...,g_R(\bm{x}))$のハミング距離を求め，最も近いカテゴリに分類できる．\par
ハミング距離$d_H$は式(\ref{eq:eq20})により求められる．ここで$\delta(a,b)$は$a = b$のとき$+1$，それ以外は$0$となるインジケータ関数である．

\begin{equation}
\label{eq:eq20}
 \hspace{20pt}d_H(\bm{W}_{c_k},\bm{g}) =\sum_{r=1}^R{\delta(W_{c_k\cdot r} , g_r(\tilde{\bm{x}}))}
\end{equation}\par

%\newpage

また，二値分類器の出力が確率値\cite{rvm} で得られるとし，$r$番目の分類器の出力を$G_r$としたとき，$\bm{x}$の推定所属カテゴリ$\hat{c}_k$は式(\ref{eq:eq1})により求めることができる．

\begin{equation}
\label{eq:eq1}
 \hspace{20pt} \hat{c}_k = \underset{{c_k}}{\text{argmin}} \prod_{r=1}^R G_r^{W_{c_k\cdot r}} (1 - G_r)^{W_{c_k\cdot r}}
\end{equation}\par

これら二つの復号方法があり，確率値として出力されるものについては，一般的にどちらの復号方法を行っても分類精度は同程度になると言われている．また，ハミング距離の欠点として，最小距離復号を行う際に複数のカテゴリで等距離になることが多いため，カテゴリを正しく推定できないことがある．

\end{description}



\newpage















